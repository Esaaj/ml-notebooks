{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92492a15",
   "metadata": {},
   "source": [
    "***Gradient Boosting Classifier*** \n",
    "\n",
    "Is a powerful machine learning algorithm It builds an ensemble of weak learners, typically decision trees, in a sequential manner where each new tree attempts to correct the errors made by the previous trees. The final model is a weighted sum of all the individual trees, which helps to improve accuracy and reduce overfitting.\n",
    "\n",
    "why 'gradient' instead of explicitly computing the residuals, the algorithm use the negative gradient of the loss function with respect to the current model's predictions as a proxy for the residuals. This approach allows for more flexibility in choosing different loss functions and can lead to better performance in certain scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of gradient boosting classifier using sklearn\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "X, y = load_boston(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "gbr.fit(X_train, y_train)\n",
    "preds = gbr.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"MSE:\", mean_squared_error(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c768dfd",
   "metadata": {},
   "source": [
    "***Key Hyperparameters***\n",
    "\n",
    "| Parameter       | Meaning                           | Typical Range                     |\n",
    "| --------------- | --------------------------------- | --------------------------------- |\n",
    "| `n_estimators`  | Number of boosting stages         | 100–1000                          |\n",
    "| `learning_rate` | Shrinks contribution of each tree | 0.01–0.1                          |\n",
    "| `max_depth`     | Tree depth                        | 3–5                               |\n",
    "| `subsample`     | % of data used per tree           | 0.5–1.0                           |\n",
    "| `loss`          | Objective function                | `squared_error`, `log_loss`, etc. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79502bf",
   "metadata": {},
   "source": [
    "Log loss measures how well a classification model predicts the probability of class membership, with lower values indicating better performance.\n",
    "\n",
    "| True Label | Predicted Probability | Log Loss (per sample) |\n",
    "|-----------:|---------------------:|----------------------:|\n",
    "| 1          | 0.9                  | 0.105 (small)         |\n",
    "| 0          | 0.1                  | 0.105 (small)         |\n",
    "| 1          | 0.8                  | 0.223 (Medium)        |\n",
    "| 1          | 0.4                  | 0.916 (Large)         |\n",
    "| **Mean**   |                      | **0.338**             |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

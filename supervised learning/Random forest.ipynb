{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e1bd48c",
   "metadata": {},
   "source": [
    "***Ensemble learning***\n",
    "\n",
    "It is a technique that combines predictions from multiple models to improve accuracy and robustness.\n",
    "\n",
    "***Bagging (Bootstrap Aggregating)***\n",
    "\n",
    "It involves training multiple models on different random subsets of the training data (with replacement) and averaging their predictions (for regression) or using majority voting (for classification).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8640035",
   "metadata": {},
   "source": [
    "***Row Sampling***\n",
    "\n",
    "Selecting a random subset of rows from the dataset to train each tree, which helps in reducing overfitting and improves model robustness. \n",
    "Typically, bootstrap sampling (sampling with replacement) is used.\n",
    "\n",
    "***Column Sampling***\n",
    "\n",
    "Selecting a random subset of features (columns) for each split in the tree, which helps in reducing correlation between trees and improves model diversity. No replacement sampling allowed.\n",
    "\n",
    "Both techniques combined lead to a more robust and generalized model by ensuring that each tree in the forest is trained on different data and features, thereby reducing variance and improving overall performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a8c0b",
   "metadata": {},
   "source": [
    "***Random forest***\n",
    "\n",
    "It builds a forest of decision trees, each trained on a random subset of the data and features, and aggregates their predictions to improve accuracy and control overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79f2f2",
   "metadata": {},
   "source": [
    "***OOB - Out of bag samples***\n",
    "\n",
    "when building each tree, about 1/3 of the data is not used (out-of-bag samples). These can be used to get an unbiased estimate of model performance without needing a separate validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71b0ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of random forest algorithm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "# For classification tasks\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# For regression tasks\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ea3f02",
   "metadata": {},
   "source": [
    "***how to find the best n_estimators***\n",
    "\n",
    "1. Cross-Validation (Most Reliable)\n",
    "\n",
    "Use cross-validation to evaluate model performance for different values of n_estimators (e.g., 50, 100, 200) and select the one that yields the best validation score.\n",
    "\n",
    "2. OOB (Out-of-Bag) Error Plot\n",
    "\n",
    "Random Forest can estimate generalization error using samples not used for training each tree.\n",
    "You can plot OOB error vs. number of trees \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3bee84",
   "metadata": {},
   "source": [
    "***Rule of thumb***\n",
    "\n",
    "| Dataset Size         | Recommended Trees |\n",
    "| -------------------- | ----------------- |\n",
    "| Small (<10K samples) | 100–200           |\n",
    "| Medium (10K–100K)    | 200–500           |\n",
    "| Large (>100K)        | 500–1000          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f53077e",
   "metadata": {},
   "source": [
    "***Implementation of Random Forest***\n",
    "\n",
    "1. Define the problem & target metrics  \n",
    "    - Identify whether it’s classification or regression, and decide evaluation metrics (accuracy, F1, RMSE, R², etc.).\n",
    "\n",
    "2. Perform quick EDA (Exploratory Data Analysis)  \n",
    "    - Check distributions, missing values, outliers, and class balance (for classification).\n",
    "\n",
    "3. Preprocess the data  \n",
    "    - Handle missing values, encode categorical features, normalize/standardize if needed, and apply basic feature engineering.\n",
    "\n",
    "4. Split the dataset  \n",
    "    - Use train_test_split; for classification, use stratify=y to maintain class ratios.\n",
    "\n",
    "5. Train a baseline Random Forest model  \n",
    "    - Start with default hyperparameters to get an initial performance benchmark.\n",
    "\n",
    "6. Cross-validation & imbalance handling  \n",
    "    - Apply KFold / StratifiedKFold; handle imbalance via class_weight='balanced' or SMOTE (for classification).\n",
    "\n",
    "7. Hyperparameter tuning  \n",
    "    - Optimize key parameters such as n_estimators, max_depth, max_features, min_samples_split, and min_samples_leaf using GridSearchCV or RandomizedSearchCV.\n",
    "\n",
    "8. Train the final model & evaluate  \n",
    "    - Retrain using the best parameters; evaluate on the test set using suitable metrics for your task.\n",
    "\n",
    "9. Interpret the model  \n",
    "    - Analyze feature importance and SHAP values to understand which features drive predictions.\n",
    "\n",
    "10. Serialize & monitor in production  \n",
    "     - Save using joblib or pickle; track drift, feature importance changes, and model performance over time.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

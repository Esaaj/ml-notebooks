{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f64411d",
   "metadata": {},
   "source": [
    "### ***Decision tree*** ### \n",
    "Is a supervised learning algorithm used for classification and regression tasks.\n",
    "\n",
    "It works by recursively splitting the data into subsets based on feature values, \n",
    "\n",
    "creating a tree-like structure where each internal node represents a decision based on a feature, \n",
    "\n",
    "each branch represents the outcome of that decision, \n",
    "\n",
    "and each leaf node represents a class label (for classification) or a continuous value (for regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e447cc6",
   "metadata": {},
   "source": [
    "Key Terminology\n",
    "\n",
    "Root Node → the first split in the tree (based on the most important feature).\n",
    "\n",
    "Splitting → dividing data into subsets.\n",
    "\n",
    "Leaf/Terminal Node → final prediction (class/value).\n",
    "\n",
    "Decision Node → when further splitting is possible.\n",
    "\n",
    "Depth → how many levels the tree has."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d0588",
   "metadata": {},
   "source": [
    "**How Does It Decide Splits?**\n",
    "\n",
    "The tree needs a rule to decide which feature to split on:\n",
    "\n",
    "***Classification Trees (CART):***\n",
    "\n",
    "Use Gini Impurity or Entropy (Information Gain).\n",
    "\n",
    "***Regression Trees:***\n",
    "\n",
    "Use Variance Reduction or Mean Squared Error (MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae47fdde",
   "metadata": {},
   "source": [
    "***heterogenity*** is when a node contains samples from multiple classes.\n",
    "\n",
    "***homogeneity*** is when a node contains samples from a single class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a89d70",
   "metadata": {},
   "source": [
    "***Entropy*** measures the uncertainty or impurity in a dataset. In decision trees, it helps determine the best feature to split on by calculating the information gain from each potential split. The feature that results in the highest information gain (i.e., the greatest reduction in entropy) is chosen for the split.\n",
    "\n",
    "Entropy is calculated as:\n",
    "```math\n",
    "Entropy(S) = - Σ (p(x) * log2(p(x)))\n",
    "```\n",
    "where p(x) is the proportion of instances belonging to class x in dataset S.\n",
    "\n",
    "***Information Gain (IG)*** is the reduction in entropy after a dataset is split on a feature. It quantifies how much uncertainty in the target variable is reduced by knowing the value of the feature.\n",
    "```math\n",
    "IG = Entropy(Parent) - [Weighted Average] * weighted children Entropy(Children)\n",
    "```\n",
    "Where:\n",
    "- Entropy(Parent) is the entropy of the dataset before the split.\n",
    "- weighted children Entropy is the entropy of the dataset after the split.\n",
    "- weighted average accounts for the proportion of samples in each child node.\n",
    "\n",
    "***weighted child entropy*** \n",
    "```math\n",
    "    weighted child entropy = (n_left / n) * Entropy(left) + (n_right / n) * Entropy(right)\n",
    "```\n",
    "where n is the total number of samples, \n",
    "n_left is the number of samples in the left child, \n",
    "n_right is the number of samples in the right child."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37340d2",
   "metadata": {},
   "source": [
    "***Gini Impurity*** same as Entropy but computationally more efficient.\n",
    "```math\n",
    "gini Impurity = 1 - Σ (p_i)^2\n",
    "```\n",
    "where p_i is the proportion of samples belonging to class i in the node.\n",
    "\n",
    "Gini Impurity ranges from 0 (pure node) to 0.5 (maximum impurity for binary classification).\n",
    "\n",
    "Entropy range from 0 (pure node) to 1 (maximum impurity for n classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e6623",
   "metadata": {},
   "source": [
    "***How Splits Work in Regression Trees***\n",
    "\n",
    "Unlike classification trees (Gini or Entropy), regression trees try to predict a continuous value.\n",
    "So, the split criterion is based on variance reduction or Mean Squared Error (MSE).\n",
    "\n",
    "Variance Reduction (or Reduction in MSE)\n",
    "\n",
    "When considering a split:\n",
    "\n",
    "1. Compute MSE before split (parent node).\n",
    "\n",
    "2. Compute weighted MSE after split (child nodes):\n",
    "```math\n",
    "MSE split​ = (n_left * mse_left + n_right * mse_right) / (n_left + n_right)\n",
    "```\n",
    "\n",
    "```math\n",
    "Variance Reduction = MSE parent ​− MSE split​\n",
    "```\n",
    "3. The algorithm tries all possible splits on all features.\n",
    "\n",
    "4. Choose the split that gives maximum variance reduction (i.e., reduces error the most)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa84dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create Decision Tree Regressor model\n",
    "model = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "# where max_depth is a hyperparameter to control overfitting\n",
    "# random_state is set for reproducibility\n",
    "# reproducibility means that every time you run the code, you get the same results\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create Decision Tree Classifier model\n",
    "model = DecisionTreeClassifier(max_depth=5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234d56f",
   "metadata": {},
   "source": [
    "***Combined pipeline checklist for Decision Trees (supports both classification & regression)***\n",
    "\n",
    "1. Load data: pd.read_csv / read_parquet / from SQL; set index; parse dates when needed.  \n",
    "2. Initial EDA: df.head(), df.shape, df.dtypes, df.isnull().sum(), df.describe(), value_counts() for categorical cols, inspect target distribution (hist / countplot) and pairwise correlations for numeric features.  \n",
    "3. Determine task: classification if target is categorical / small integer classes; regression if continuous.  \n",
    "4. Preprocess:  \n",
    "    - Missing values: impute (median/mean for numeric, mode/constant for categorical) or model-based imputation.  \n",
    "    - Convert dtypes: to_numeric, to_datetime, categorical dtype for categories.  \n",
    "    - Outliers: clip, transform (log), or model-robust approaches.  \n",
    "5. Encoding:  \n",
    "    - Classification: one-hot for nominal, ordinal encoding for ordered categories.  \n",
    "    - Regression: same encodings; consider target encoding for high-cardinality features with caution.  \n",
    "6. Feature engineering / selection: create domain features, interactions, select with feature importance or model-based selection.  \n",
    "7. Split data: train / val / test (stratify for classification when appropriate).  \n",
    "8. Choose model:  \n",
    "    - Classification: sklearn.tree.DecisionTreeClassifier (consider class_weight='balanced' for imbalanced data).  \n",
    "    - Regression: sklearn.tree.DecisionTreeRegressor.  \n",
    "    - Common hyperparams: max_depth, min_samples_leaf, min_samples_split, max_features, random_state.  \n",
    "9. Train on training set.  \n",
    "10. Evaluate:  \n",
    "     - Classification metrics: accuracy, precision, recall, F1, ROC AUC, confusion matrix.  \n",
    "     - Regression metrics: MSE, RMSE, MAE, R², residual analysis.  \n",
    "11. Cross-validation: use cross_val_score or cross_validate with appropriate scoring (scoring='neg_root_mean_squared_error' or 'roc_auc', etc.).  \n",
    "12. Hyperparameter tuning: GridSearchCV / RandomizedSearchCV (or Bayesian optimizers) with cv and appropriate scoring.  \n",
    "13. Regularization/pruning: tune max_depth, min_samples_leaf, ccp_alpha (cost-complexity pruning).  \n",
    "14. Diagnostics: inspect residuals (regression), calibration & confusion matrix (classification), and feature_importances_.  \n",
    "15. Visualization: sklearn.tree.plot_tree or export_graphviz; prediction vs actual plots; partial dependence plots.  \n",
    "16. Persist model: joblib.dump / pickle; encapsulate preprocessing + model into a pipeline (sklearn.pipeline.Pipeline).  \n",
    "17. Deployment & monitoring: serve model, monitor drift, retrain when performance degrades.  \n",
    "18. Optional helper to choose model programmatically:\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e40ce917",
   "metadata": {},
   "source": [
    "***Extreme gradient boosting (XGBoost)***\n",
    "\n",
    "Is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.\n",
    "\n",
    "This uses Ridge/lasso regularaization where gradient uses a minimal regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481cf9c6",
   "metadata": {},
   "source": [
    "It’s an improved implementation of gradient boosting, focused on:\n",
    "\n",
    "1. Performance (fast)\n",
    "\n",
    "2. Regularization (to avoid overfitting)\n",
    "\n",
    "3. Handling missing values\n",
    "\n",
    "4. Parallelization (CPU & GPU support)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3afc5ae",
   "metadata": {},
   "source": [
    "***Key Concepts Behind XGBoost***\n",
    "\n",
    "1. Same foundation as Gradient Boosting\n",
    "\n",
    "    - Builds trees sequentially.\n",
    "\n",
    "    - Each tree corrects the errors of the previous ones.\n",
    "\n",
    "    - Uses gradient descent to minimize the loss.\n",
    "\n",
    "2. But adds several improvements:\n",
    "\n",
    "| Feature             | Gradient Boosting | XGBoost                                            |\n",
    "| ------------------- | ----------------- | -----------------------------------------------    |\n",
    "| Regularization      | None or minimal   | ✅ L1 & L2 regularization (like Ridge/Lasso)      |\n",
    "| Parallelization     | ❌ Sequential      | ✅ Parallel tree construction                    |\n",
    "| Missing values      | ❌ Manual handling | ✅ Auto-learns best direction for missing values |\n",
    "| Overfitting control | ❌ Manual          | ✅ Shrinkage + Subsampling                       |\n",
    "| Tree growth         | Level-wise        | ✅ Depth-wise + best split per leaf                |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0790ae",
   "metadata": {},
   "source": [
    "***How XGBoost Works (Step-by-Step)***\n",
    "\n",
    "1. Initialize model with a constant prediction (e.g., mean of target).\n",
    "\n",
    "2. For each iteration:\n",
    "\n",
    "    - Compute the gradient (first derivative of loss) and hessian (second derivative).\n",
    "\n",
    "    - Fit a new tree to the gradients (i.e., residuals).\n",
    "\n",
    "    - Compute the leaf weights using both gradients and hessians.\n",
    "\n",
    "    - Update predictions with a learning rate (η).\n",
    "\n",
    "3. Final prediction = sum of all weighted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03604caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of xg boost model from sklearn\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model\n",
    "model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125e12b1",
   "metadata": {},
   "source": [
    "***Important Hyperparameters***\n",
    "\n",
    "| Parameter          | Description                                        |\n",
    "| ------------------ | -------------------------------------------------- |\n",
    "| `n_estimators`     | Number of boosting rounds (trees)                  |\n",
    "| `learning_rate`    | Shrinks contribution of each tree                  |\n",
    "| `max_depth`        | Maximum depth of each tree                         |\n",
    "| `subsample`        | Fraction of data used per tree                     |\n",
    "| `colsample_bytree` | Fraction of features used per tree                 |\n",
    "| `lambda`           | L2 regularization                                  |\n",
    "| `alpha`            | L1 regularization                                  |\n",
    "| `gamma`            | Minimum loss reduction to make a further partition |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

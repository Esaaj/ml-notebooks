{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45f27995",
   "metadata": {},
   "source": [
    "### ***Linear regression***\n",
    "\n",
    "When we try to find a line such that most of the points are close to the line.\n",
    "\n",
    "\n",
    "```math\n",
    "y = m x + c\n",
    "```\n",
    "\n",
    "**Goal:** Model relationship between independent variables (X) and a dependent variable (y).\n",
    "Predict continuous outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc7bfb",
   "metadata": {},
   "source": [
    "### ***Multiple Linear Regression***\n",
    "\n",
    "when we have multiple independent variables (features) to predict a dependent variable (target).\n",
    "```math\n",
    "    y = m1​x1​ + m2​x2 ​+ m3​x3​ + ⋯ + mn​xn ​+ c + ε\n",
    "```\n",
    "\n",
    "where:\n",
    "    - y is the dependent variable (target).\n",
    "    - x1, x2, ..., xn are the independent variables (features).\n",
    "    - m1, m2, ..., mn are the coefficients (slopes) for each independent variable.\n",
    "    - c is the y-intercept (constant term).\n",
    "    - ε is the error term (residuals)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db529601",
   "metadata": {},
   "source": [
    "***Mean Squared Error (MSE)***\n",
    "    \n",
    "The Mean Squared Error (MSE) is a common metric used to evaluate the performance of regression models. \n",
    "It measures the average of the squares of the errors—that is, the average squared difference between the actual and predicted values.\n",
    "```math\n",
    "        mse = (1/n) * Σ(actual - predicted)²\n",
    "```\n",
    "This is very bad for outliers.\n",
    "For example error before averaging means 10 will be 100 after squaring.\n",
    "\n",
    "***Mean Absolute Error (MAE)***\n",
    "\n",
    "The Mean Absolute Error (MAE) is another metric used to evaluate the performance of regression models.\n",
    "It measures the average of the absolute differences between the actual and predicted values.\n",
    "```math\n",
    "        mae = (1/n) * Σ|actual - predicted|\n",
    "```\n",
    "MAE is more robust to outliers compared to MSE because it does not square the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c654f50",
   "metadata": {},
   "source": [
    "***Residual sum of squares (RSS)***\n",
    "\n",
    "This tells us how well our model is able to predict the target variable.\n",
    "It is the sum of the squares of the differences between the actual and predicted values.\n",
    "The lower the RSS, the better the model fits the data.\n",
    "```math\n",
    "RSS = Σ (yi - ŷi)²\n",
    "```\n",
    "where yi is the actual value and ŷi is the predicted value.\n",
    "\n",
    "***Total sum of squares (TSS)***\n",
    "This measures the total variability in the target variable.\n",
    "It is the sum of the squares of the differences between the actual values and the mean of the actual values.\n",
    "```math\n",
    "TSS = Σ (yi - ȳ)²\n",
    "```\n",
    "where yi is the actual value and ȳ is the mean of the actual values.\n",
    "\n",
    "***R-squared (R²)***\n",
    "This is a statistical measure that represents the proportion of the variance for the target variable that is explained by the independent variables in the model.\n",
    "It is calculated as:\n",
    "```math\n",
    "R² = 1 - (RSS / TSS)\n",
    "```\n",
    "R² ranges from 0 to 1, where:\n",
    "- 0 indicates that the model does not explain any of the variability in the target variable.\n",
    "- 1 indicates that the model explains all the variability in the target variable.\n",
    "\n",
    "A higher R² value indicates a better fit of the model to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573092fc",
   "metadata": {},
   "source": [
    "***Root Mean Squared Error (RMSE)***\n",
    "RMSE is the square root of the average squared differences between predicted and actual values.\n",
    "```math\n",
    "    rmse = √( (1/n) * Σ(actual - predicted)² )\n",
    "```\n",
    "RMSE gives higher weight to large errors and is in the same unit as the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4127a5f",
   "metadata": {},
   "source": [
    "***Adjusted R²***\n",
    "Adjusted R² adjusts the R² value based on the number of predictors.\n",
    "It prevents overestimating the goodness of fit when adding more variables.\n",
    "```math\n",
    "        Adjusted R² = 1 - (1 - R²) * (n - 1) / (n - p - 1)\n",
    "```\n",
    "\n",
    "where:\n",
    "    n = number of observations\n",
    "    p = number of independent variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1947ed47",
   "metadata": {},
   "source": [
    "***Multicollinearity***\n",
    "\n",
    "When one independent variable (feature) in a regression model can be (nearly) linearly predicted from the others, there is multicollinearity.\n",
    "\n",
    "***Example:***\n",
    "    c1 = c2 + c3 - c4\n",
    "\n",
    "Some value of columns can be identified easily like total but some of the columns are little difficult to identify\n",
    "So we create linear regression for each independent variable using the rest of the values\n",
    "\n",
    "Now R² of this model predict is very low then that independent variable has no multicollinearity with any other variable.\n",
    "\n",
    "***Others way to find Multicollinearity***\n",
    "Pairwise correlation matrix (high absolute correlations are a hint).\n",
    "Variance Inflation Factor (VIF)\n",
    "```math\n",
    "    VIF = 1 / 1 - R²\n",
    "```\n",
    "we can calculate VIF for all the variables when VIF is high we cant use that variable\n",
    "common rule-of-thumb: VIF > 5 or 10 signals concern.\n",
    "\n",
    "***Common remedies:***\n",
    "- Remove or combine highly correlated predictors (e.g., drop one, sum, or average).\n",
    "- Use dimensionality reduction: Principal Component Regression (PCR) or Partial Least Squares (PLS).\n",
    "- Apply regularization: Ridge (L2) reduces variance; Lasso (L1) can perform variable selection.\n",
    "\n",
    "***Notes:***\n",
    "- Multicollinearity mainly affects interpretability of coefficients, not necessarily predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40235ba",
   "metadata": {},
   "source": [
    "***Assumptions of Linear Regression:***\n",
    "1. Linearity: The relationship between the independent and dependent variables is linear.\n",
    "2. Independence: Observations are independent of each other.\n",
    "3. Homoscedasticity: Constant variance of errors across all levels of the independent variable.\n",
    "4. Normality of Errors: The residuals (errors) are normally distributed.\n",
    "5. No Multicollinearity: Independent variables are not highly correlated with each other.\n",
    "6. No Autocorrelation: Residuals are not correlated with each other, especially in time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18f37d",
   "metadata": {},
   "source": [
    "***Implementation of linear regression***\n",
    "```code\n",
    "from sklearn import linear_model\n",
    "model = linear_model.LinearRegression()\n",
    "```\n",
    "different parameters avalilable in linear regression\n",
    "```code\n",
    "model = linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None, positive=False)\n",
    "```\n",
    "\n",
    "***fit_intercept:*** whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (e.g. data is expected to be already centered).\n",
    "\n",
    "***normalize:*** This parameter is ignored when fit_intercept is set to False. If True,\n",
    "    the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm.\n",
    "\n",
    "***copy_X:*** If True, X will be copied; else, it may be overwritten.\n",
    "\n",
    "***n_jobs:*** The number of jobs to use for the computation. This will only provide speed\n",
    "    up the computation if multiple input arrays are passed to the fit method of the estimator\n",
    "    and if n_targets is greater than 1. None means 1 unless in a joblib.parallel_backend context.\n",
    "    \n",
    "***positive:*** When set to True, forces the coefficients to be positive. This option is only supported for dense arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e2f16",
   "metadata": {},
   "source": [
    "***Order of implementation***\n",
    "\n",
    "1. Import necessary libraries\n",
    "2. Load the dataset\n",
    "3. data processing\n",
    "4. Check for correlations (heatmap)\n",
    "5. Detect multicollinearity (VIF)\n",
    "6. Split data into train and test sets\n",
    "7. Apply k-Fold Cross-Validation (e.g., K=5 or 10)\n",
    "8. Fit Linear Regression model on training data\n",
    "9. Predict on test data\n",
    "10. Evaluate model — MSE, RMSE, MAE, R², Adjusted R²\n",
    "11. Plot residuals (check assumptions visually)\n",
    "12. Check model assumptions (linearity, homoscedasticity, normality)\n",
    "13. Interpret coefficients (feature importance)\n",
    "14. Apply Regularization (Ridge / Lasso / ElasticNet) — optional\n",
    "15. Save or deploy the model — optional"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

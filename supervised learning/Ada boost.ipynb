{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23331b38",
   "metadata": {},
   "source": [
    "***Boosting***\n",
    "\n",
    "It is a ensumble technique that combines multiple weak learners (typically decision trees) to create a strong predictive model.\n",
    "\n",
    "A weak learner is a model that performs slightly better than random guessing. In the context of boosting, decision trees are often used as weak learners because they can capture complex patterns in the data while being relatively simple and fast to train.\n",
    "\n",
    "The key idea behind boosting is to train a sequence of weak learners, where each subsequent learner focuses on the mistakes made by the previous ones. This is typically done by assigning higher weights to the misclassified instances in the training data, so that the next learner pays more attention to those difficult cases.\n",
    "\n",
    "The final prediction is made by combining the predictions of all the weak learners, often through a weighted majority vote (for classification) or a weighted average (for regression). The weights are usually determined based on the performance of each learner, with better-performing learners receiving higher weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feca352",
   "metadata": {},
   "source": [
    "***ADA BOOSTING***\n",
    "\n",
    "Adapative boosting combines multiple weak learners (typically decision trees) to create a strong predictive model.\n",
    "Each subsequent learner focuses on the mistakes made by the previous ones, improving overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cd91dc",
   "metadata": {},
   "source": [
    "Let’s say you train 3 weak models:\n",
    "| Model | Accuracy | What happens next                          |\n",
    "| ----- | -------- | ------------------------------------------ |\n",
    "| 1st   | 60%      | Increase weights of misclassified points   |\n",
    "| 2nd   | 70%      | Focus more on the previously wrong samples |\n",
    "| 3rd   | 85%      | Combines all models → strong ensemble      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33571db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of ada boosting algorithm\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Base learner (weak model)\n",
    "base_model = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# AdaBoost\n",
    "ada = AdaBoostClassifier(\n",
    "    estimator=base_model,\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb51a2e",
   "metadata": {},
   "source": [
    "***Key Hyperparameters***\n",
    "\n",
    "| Parameter       | Meaning                              | Typical Range                                 |\n",
    "| --------------- | ------------------------------------ | --------------------------------------------- |\n",
    "| `n_estimators`  | Number of weak learners              | 50–500                                        |\n",
    "| `learning_rate` | Shrinks contribution of each learner | 0.01–1.0                                      |\n",
    "| `estimator`     | Base weak learner                    | Usually `DecisionTreeClassifier(max_depth=1)` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ea191",
   "metadata": {},
   "source": [
    "***order of implementation for ada boost algorithm***\n",
    "\n",
    "1. Define problem & goal  \n",
    "    - Classification or regression?  \n",
    "    - Choose metrics (Accuracy/F1 for classification, MAE/RMSE for regression).\n",
    "\n",
    "2. Perform EDA (Exploratory Data Analysis)  \n",
    "    - Check data distribution, missing values, class imbalance, and feature correlations.\n",
    "\n",
    "3. Preprocess the data  \n",
    "    - Handle missing values, encode categorical variables, scale numerical features if needed.\n",
    "\n",
    "4. Split dataset  \n",
    "    - Use train_test_split; for classification, use stratify=y to maintain class balance.\n",
    "\n",
    "5. Select base estimator  \n",
    "    - Usually a DecisionTreeClassifier (max_depth=1) — a weak learner (called a “stump”).\n",
    "\n",
    "6. Train baseline AdaBoost model  \n",
    "    - Initialize AdaBoostClassifier or AdaBoostRegressor with default parameters.\n",
    "\n",
    "7. Cross-validation  \n",
    "    - Use KFold / StratifiedKFold to check stability across folds.\n",
    "\n",
    "8. Hyperparameter tuning  \n",
    "    - Tune parameters such as:  \n",
    "      - n_estimators (number of weak learners)  \n",
    "      - learning_rate (step size per learner)  \n",
    "      - base estimator depth (max_depth for stump)\n",
    "\n",
    "9. Train final model & evaluate  \n",
    "    - Retrain with best parameters and evaluate using chosen metrics.  \n",
    "    - Plot learning curves or feature importances.\n",
    "\n",
    "10. Interpret results  \n",
    "     - Analyze misclassifications, feature importance, and contribution of weak learners.\n",
    "\n",
    "11. Save & monitor  \n",
    "     - Serialize model (joblib, pickle), and monitor performance over time.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
